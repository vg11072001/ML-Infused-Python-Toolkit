# Pyspark

![image](https://github.com/vg11072001/Python-programming/assets/67424390/62030424-afba-40e1-b52c-cab98442a94b)

### Concept and Structure
* Application: propgram built using APIs, driver & excutor
* SparkSession: Crete spark driver, intitaes sesssion and during it own spark objects (Driver is part of spark shell)
* Job: parallel comptutaion, it gets distributed
* Stage
* Task: Single unit of work

![image](https://github.com/vg11072001/Python-programming/assets/67424390/f6a8778b-9c95-47ca-b1c8-0d7f9c4e6c62)

###### Application to Mutiple job using Driver 

![image](https://github.com/vg11072001/Python-programming/assets/67424390/28ca126e-62ad-4440-87e0-15208bb27b39)

###### Tranforms, Actions and Lazy Evaluation

![image](https://github.com/vg11072001/Python-programming/assets/67424390/3794cc6a-ee9e-4540-a5d2-d43ee25ea63a)

![image](https://github.com/vg11072001/Python-programming/assets/67424390/3ec26e8f-88f7-4a3a-9eab-6ae5eb52afcd)

#### Spark UI (holistic view of your applications, to easliy monitor)
* List of scheduler stages and tasks
* Summary of RDD sizes and memory usage
* Information about the environment
* Information about the running executors
* All Spark SQL queries

#### RDD (Resilient Distributed Datasets)


![image](https://github.com/vg11072001/Python-programming/assets/67424390/a5133bc8-a941-4c58-a80f-ba880409183f)


### Code 

#### 1 start

SparkAPP

![image](https://github.com/vg11072001/Python-programming/assets/67424390/ea57f370-0a3c-489a-86a3-df52837172c2)
![image](https://github.com/vg11072001/Python-programming/assets/67424390/588a3c5c-47b6-4c4e-86c1-911dc78c99c4)

